# 基于Flink的淘宝实时数据分析可视化

## 题外话1：如何把这个项目跑起来

```shell
#1、启动zookeeper
myzookeeper.sh start
#2、启动kafka
mykafka.sh start
#3、启动需要的conda虚拟环境
conda activate pyspark #pyflink和pyspark的依赖包都安装在该虚拟环境中，apache-flink版本为1.17.1
#4、进入项目所在位置
cd /opt/module/code/Taobao_Realtime2.0
#5、启动flask应用
python app_flink.py
#6、打开一个新的会话
conda activate pyspark
cd /opt/module/code/Taobao_Realtime2.0
python producer.py #启动生产者
#7、打开本机浏览器(虚拟机或主机都可以)进入web前端 实时前端可视化
hadoop105:5001
```

## 题外话2：采用的数据集字段说明

|   字段名    | 数据类型 | 取值范围 |                             说明                             |
| :---------: | :------: | :------: | :----------------------------------------------------------: |
|   user_id   |  String  |    -     |                        用户唯一标识符                        |
|   item_id   |  String  |    -     |                        商品唯一标识符                        |
|   cat_id    |  String  |    -     |                          商品类别ID                          |
| merchant_id |  String  |    -     |                        商家唯一标识符                        |
|  brand_id   |  String  |    -     |                        品牌唯一标识符                        |
|    month    | Integer  |   1-12   |                             月份                             |
|     day     | Integer  |   1-31   |                             日期                             |
|   action    | Integer  |   0-3    | 用户行为类型：<br>0: 点击<br>1: 加购物车<br>2: 购买<br>3: 关注 |
|  age_range  | Integer  |   0-7    | 用户年龄段：<br>0: 未知<br>1: <20<br>2: [20-25)<br>3: [25-30)<br>4: [30-35)<br>5: [35-40)<br>6: [40-50)<br>7: ≥50 |
|   gender    | Integer  |   0-2    |           用户性别：<br>0: 女<br>1: 男<br>2: 未知            |
|  province   |  String  |    -     |                         用户所在省份                         |

## 1. 项目概述

### 1.1 项目简介
淘宝实时数据分析平台是一个基于Apache Flink和Kafka的流式大数据处理系统，专门针对电商用户行为数据进行实时分析和可视化展示。该平台能够处理高吞吐量的用户行为事件流，提供多维度的实时统计分析，并通过Web界面进行实时数据大屏展示。

### 1.2 核心功能
- **实时数据摄取**：基于Kafka消息队列进行高并发数据接入
- **流式数据处理**：利用Flink进行低延迟的实时数据处理
- **多维度分析**：支持用户行为、年龄分布、性别比例、地域分布等多维度统计
- **实时可视化**：提供Web大屏界面，实时展示分析结果 http://hadoop105:5001/
- **事件时间处理**：支持事件时间语义和水位线机制
- **迟到数据处理**：完善的迟到数据检测和处理机制

## 2. 系统架构设计

### 2.1 整体架构
```
数据源(CSV) → Kafka Producer → Kafka Cluster → Flink Consumer → flink实时计算结果 → Flask后端 → WebSocket → 前端大屏
```

### 2.2 技术栈
- **消息中间件**：Apache Kafka
- **流处理引擎**：Apache Flink 1.17.1 (PyFlink)
- **Web框架**：Flask + Flask-SocketIO
- **前端技术**：HTML5 + ECharts + WebSocket
- **数据可视化**：PyECharts + ECharts
- **开发语言**：Python 3.x

### 2.3 系统组件

#### 2.3.1 数据生产者（Producer）
- **文件位置**：`producer.py`
- **功能**：从CSV文件读取淘宝用户行为数据，发送至Kafka主题
- **核心特性**：
  - 支持CSV文件批量读取
  - 可配置的发送速率控制
  - 完善的错误处理和日志记录
  - 自动重试机制

```python
# Producer核心配置
producer = KafkaProducer(
    bootstrap_servers=['hadoop105:9092'],
    value_serializer=lambda x: json.dumps(x).encode('utf-8'),
    security_protocol="PLAINTEXT",
    api_version_auto_timeout_ms=30000,
    request_timeout_ms=30000
)

# 发送速率控制
time.sleep(0.1)  # 控制发送速率，避免Kafka压力过大
```

#### 2.3.2 流处理消费者（Flink Consumer）
- **文件位置**：`flink_consumer.py`
- **功能**：从Kafka消费数据，进行实时流处理和统计分析
- **核心组件**：
  - `EventTimeExtractor`：事件时间提取器
  - `WindowedDataProcessor`：窗口数据处理器
  - `EnhancedDataProcessor`：增强型数据处理器

```python
# Flink环境配置
env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
env.enable_checkpointing(5000)  # 5秒检查点间隔

# Kafka消费者配置
kafka_props = {
    'bootstrap.servers': 'hadoop105:9092',
    'group.id': 'flink-enhanced-group',
    'auto.offset.reset': 'latest'
}
```

#### 2.3.3 Web应用服务（Flask App）
- **文件位置**：`app_flink.py`
- **功能**：提供Web界面和WebSocket服务，实时推送数据
- **特性**：
  - 实时图表生成
  - WebSocket双向通信
  - 多维度数据映射和展示
  - 支持数据导出功能

```python
# Flask-SocketIO配置
socketio = SocketIO(
    app,
    cors_allowed_origins="*",
    async_mode='threading',
    ping_timeout=60,
    ping_interval=25,
    max_http_buffer_size=1e8
)
```

### 2.4 依赖管理
项目使用`requirements.txt`管理Python依赖：

```txt
flask                    # Web框架
flask-socketio          # WebSocket支持
python-socketio         # Socket.IO Python实现
python-engineio         # Engine.IO Python实现
pandas                  # 数据处理
pyecharts               # 图表生成
gevent-websocket        # WebSocket服务器
gevent                  # 异步网络库
pymysql                 # MySQL数据库连接
dbutils                 # 数据库连接池
```

## 3. 数据流架构

### 3.1 数据流路径
1. **数据摄取层**：CSV数据源 → Kafka Producer → Kafka Topic
2. **流处理层**：Kafka Consumer → Flink DataStream → 事件时间处理 → 窗口计算
3. **数据存储层**：MySQL数据库 → 连接池管理 → 数据持久化
4. **展示层**：Flask WebSocket → 前端图表 

### 3.2 数据集字段介绍
```json
{
  "user_id": "用户ID",
  "item_id": "商品ID", 
  "cat_id": "商品类别ID",
  "merchant_id": "商家ID",
  "brand_id": "品牌ID",
  "month": "月份",
  "day": "日期",
  "action": "用户行为(0:点击,1:加购,2:购买,3:关注)",
  "age_range": "年龄段(0-7)",
  "gender": "性别(0:女,1:男,2:未知)",
  "province": "省份"
}
```

### 3.3 数据映射配置
系统内置了完整的数据映射字典，用于前端展示：

```python
# 用户行为映射
ACTION_MAP = {
    '0': '点击',
    '1': '加入购物车',
    '2': '购买',
    '3': '关注商品'
}

# 年龄段映射
AGE_MAP = {
    '1': '年龄<18',
    '2': '18-24岁',
    '3': '25-29岁',
    '4': '30-34岁',
    '5': '35-39岁',
    '6': '40-49岁',
    '7': '年龄>=50',
    '0': '未知'
}

# 性别映射
GENDER_MAP = {
    '0': '女性',
    '1': '男性',
    '2': '未知'
}
```

## 4. 数据管道设计

### 4.1 事件时间提取
- **实现类**：`EventTimeExtractor`
- **策略**：基于数据中的月份和日期字段构造事件时间戳
- **回退机制**：当日期字段无效时，使用当前系统时间
- **时间戳格式**：毫秒级Unix时间戳

```python
class EventTimeExtractor(MapFunction):
    """提取事件时间并添加时间戳"""
    
    def map(self, value):
        try:
            data = json.loads(value)
            
            # 从数据中提取时间信息，构造事件时间
            if 'month' in data and 'day' in data:
                current_year = datetime.now().year
                try:
                    event_time = datetime(current_year, int(data['month']), int(data['day']))
                    event_timestamp = int(event_time.timestamp() * 1000)
                except:
                    # 如果日期无效，使用当前时间
                    event_timestamp = int(time.time() * 1000)
            else:
                # 如果没有时间字段，使用当前时间
                event_timestamp = int(time.time() * 1000)
            
            # 添加时间戳信息
            data['event_timestamp'] = event_timestamp # 事件时间
            data['processing_timestamp'] = int(time.time() * 1000) # 处理时间
            
            return json.dumps(data)
            
        except Exception as e:
            logger.error(f"提取事件时间失败: {str(e)}")
            # 返回原始数据，添加当前时间戳
            try:
                data = json.loads(value)
                data['event_timestamp'] = int(time.time() * 1000)
                data['processing_timestamp'] = int(time.time() * 1000)
                return json.dumps(data)
            except:
                return value
```

### 4.2 水位线配置

- **水位线策略**：基于事件时间的单调递增水位线
- **延迟容忍度**：5分钟（300秒）
- **更新频率**：跟随事件流动态更新

```python
# 水位线配置示例
watermark_strategy = WatermarkStrategy \
    .for_bounded_out_of_orderness(Duration.of_seconds(300)) \
    .with_timestamp_assigner(lambda event, timestamp: event['event_timestamp'])
```

### 4.3 迟到数据处理

#### 4.3.1 迟到数据检测
- **实现类**：`WindowedDataProcessor`
- **检测逻辑**：比较事件时间与处理时间的差值
- **迟到阈值**：5分钟（300秒）
- **处理策略**：标记但不丢弃，独立统计

```python
class WindowedDataProcessor(MapFunction):
    """基于窗口的数据处理器"""
    
    def __init__(self):
        self.late_data_count = 0
        self.processed_count = 0
    
    def map(self, value):
        try:
            data = json.loads(value)
            self.processed_count += 1
            
            # 检查是否是迟到数据
            current_time = int(time.time() * 1000)
            event_time = data.get('event_timestamp', current_time)
            
            # 计算延迟时间
            delay_seconds = (current_time - event_time) / 1000
            
            # 如果事件时间比当前时间早5分钟以上，认为是迟到数据
            if delay_seconds > 300:  # 5分钟阈值
                self.late_data_count += 1
                logger.warning(f"检测到迟到数据 #{self.late_data_count}, 延迟: {delay_seconds:.2f}秒")
                # 标记为迟到数据但仍然处理
                data['is_late'] = True
                data['delay_seconds'] = delay_seconds
                return json.dumps(data)
            
            # 正常数据处理
            data['is_late'] = False
            data['delay_seconds'] = delay_seconds
            
            if self.processed_count % 100 == 0:  # 每100条记录输出一次统计
                logger.info(f"已处理 {self.processed_count} 条记录，其中迟到数据 {self.late_data_count} 条")
            
            return json.dumps(data)
            
        except Exception as e:
            logger.error(f"窗口数据处理错误: {str(e)}")
            return value
```

#### 4.3.2 迟到数据处理机制
- **分离处理**：正常数据和迟到数据分别统计
- **日志记录**：详细记录迟到数据的延迟时间和数量
- **统计上报**：定期输出迟到数据统计信息

### 4.4 批处理优化
**实现类**：`EnhancedDataProcessor`
- **批处理策略**：动态调整批次大小（默认10条）
- **触发条件**：基于数量或时间窗口
- **反压控制**：防止系统过载

```python
class EnhancedDataProcessor(MapFunction):
    """数据处理器"""
    
    def __init__(self):
        self.batch_data = []
        self.batch_size = 10
        self.last_process_time = time.time()
        self.message_count = 0
        self.late_message_count = 0
        self.total_delay = 0
    
    def map(self, value):
        try:
            self.message_count += 1
            data = json.loads(value)
            
            # 记录延迟统计
            if 'delay_seconds' in data:
                self.total_delay += data['delay_seconds']
                avg_delay = self.total_delay / self.message_count
                
                if data.get('is_late', False):
                    self.late_message_count += 1
                
                if self.message_count % 50 == 0:  # 每50条消息输出一次统计
                    logger.info(f"处理统计: 总数={self.message_count}, 迟到={self.late_message_count}, 平均延迟={avg_delay:.2f}秒")
            
            # 添加到批次
            self.batch_data.append(data)
            current_time = time.time()
            
            # 批处理触发条件：达到批次大小或超过时间窗口
            if len(self.batch_data) >= self.batch_size or (current_time - self.last_process_time) > 2:
                self.process_batch()
                self.last_process_time = current_time
            
            return value
            
        except Exception as e:
            logger.error(f"数据处理错误: {str(e)}")
            return value
    
    def process_batch(self):
        """处理批次数据"""
        if not self.batch_data:
            return
        
        try:
            # 统计处理
            stats = self.calculate_stats(self.batch_data)
            
            # 发送到队列
            flink_data_queue.put(stats)
            
            # 清空批次
            self.batch_data = []
            
        except Exception as e:
            logger.error(f"批处理错误: {str(e)}")
```

## 5. 检查点配置

### 5.1 检查点设置
```python
# Flink检查点配置
env.enable_checkpointing(5000)  # 5秒间隔
```

### 5.2 容错机制
- **检查点间隔**：5秒
- **状态后端**：内存状态后端（适用于开发和小规模部署）
- **恢复策略**：从最近检查点恢复

## 6. 状态管理方案

### 6.1 状态类型
- **应用级状态**：累积统计数据（`cumulative_stats`）
- **批处理状态**：临时批次数据缓存
- **连接状态**：Kafka消费者偏移量管理

### 6.2 状态存储
- **内存存储**：全局字典结构存储累积统计
- **队列缓存**：`queue.Queue`用于线程间数据传递
- **状态更新**：增量更新和累积计算

```python
# 状态累积逻辑
def accumulate(target, incoming):
    for k, v in incoming.items():
        target[k] = target.get(k, 0) + v

# 全局状态管理
cumulative_stats = {
    'action': {},
    'age': {},
    'gender': {},
    'province': {}
}
```

## 7. 反压处理机制

### 7.1 反压检测
- **队列监控**：监控`flink_data_queue`队列长度
- **处理速率监控**：统计消息处理速率和延迟

### 7.2 反压处理策略
- **批处理优化**：动态调整批次大小（默认10条）
- **背压调节**：基于时间窗口的批处理触发
- **资源控制**：设置适当的并行度（当前为1）

```python
# 批处理反压控制
if len(self.batch_data) >= self.batch_size or (current_time - self.last_process_time) > 2:
    self.process_batch()
```

## 8. 实时交付界面

### 8.1 可视化图表
1. **用户行为分布**：饼图展示点击、加购、购买、关注行为占比
2. **年龄分布**：柱状图展示不同年龄段用户分布
3. **性别分布**：饼图展示用户性别比例
4. **地域分布**：柱状图展示各省份用户分布

### 8.2 图表生成代码
```python
def create_action_chart(data):
    """创建用户行为分布饼图"""
    try:
        if not data:
            return None
        mapped_data = {ACTION_MAP.get(k, k): v for k, v in data.items()}
        pie = (
            Pie(init_opts=opts.InitOpts(theme="dark"))
            .add(
                "",
                [list(z) for z in zip(mapped_data.keys(), mapped_data.values())],
                radius=["40%", "75%"],
            )
            .set_global_opts(
                title_opts=opts.TitleOpts(title="用户行为分布 (Flink)"),
                legend_opts=opts.LegendOpts(orient="vertical", pos_top="15%", pos_left="2%")
            )
            .set_series_opts(label_opts=opts.LabelOpts(formatter="{b}: {c}"))
        )
        return json.loads(pie.dump_options())
    except Exception as e:
        logger.error(f"Error creating action chart: {str(e)}")
        return None
```

## 9. 中间件Kafka应用

### 9.1 Kafka配置
```python
# Producer配置
producer = KafkaProducer(
    bootstrap_servers=['hadoop105:9092'],
    value_serializer=lambda x: json.dumps(x).encode('utf-8'),
    security_protocol="PLAINTEXT",
    api_version_auto_timeout_ms=30000,
    request_timeout_ms=30000
)

# Consumer配置
kafka_props = {
    'bootstrap.servers': 'hadoop105:9092',
    'group.id': 'flink-enhanced-group',
    'auto.offset.reset': 'latest'
}
```

### 9.2 主题配置
- **主题名称**：`taobao_data2`
- **分区数**：默认分区数
- **副本因子**：默认副本因子
- **保留策略**：默认保留策略

### 9.3 性能优化
- **批量发送**：支持批量消息发送
- **压缩配置**：支持消息压缩
- **重试机制**：自动重试失败的消息
- **超时配置**：合理的超时时间设置

## 10. 数据库集成

### 10.1 数据库配置
```python
# 数据库连接配置
DB_CONFIG = {
    'host': 'localhost',
    'port': 3306,
    'user': 'root',
    'password': 'password',
    'database': 'taobao_analysis',
    'charset': 'utf8mb4'
}

# 连接池配置
POOL = PooledDB(
    creator=pymysql,
    maxconnections=5,
    mincached=2,
    maxcached=5,
    blocking=True,
    host='localhost',
    user='root',
    password='xiao159205',
    database='flink_recommend',
    charset='utf8mb4',
    cursorclass=pymysql.cursors.DictCursor
)
```

### 10.2 数据持久化
- **统计结果存储**：将实时统计结果存储到MySQL
- **历史数据查询**：支持历史数据查询和导出
- **连接池管理**：使用连接池提高数据库访问效率

### 10.3 数据导出功能
```python
@app.route('/api/export_user_stats')
def api_export_user_stats():
    """导出用户统计数据"""
    try:
        csv_data = export_user_stats_to_csv()
        return send_file(
            csv_data,
            mimetype='text/csv',
            as_attachment=True,
            download_name='user_stats.csv'
        )
    except Exception as e:
        logger.error(f"导出数据失败: {str(e)}")
        return jsonify({'error': '导出失败'}), 500
```

## 11. 历史数据查询功能

### 11.1 数据库表结构
系统使用MySQL数据库存储历史统计数据，主要表结构如下：

```sql
-- 用户统计表
CREATE TABLE user_stats (
    id INT AUTO_INCREMENT PRIMARY KEY,
    window_time DATETIME COMMENT '统计窗口时间',
    action_type VARCHAR(20) COMMENT '行为类型',
    action_count INT COMMENT '行为次数',
    age_range VARCHAR(20) COMMENT '年龄段',
    age_count INT COMMENT '年龄统计',
    gender VARCHAR(10) COMMENT '性别',
    gender_count INT COMMENT '性别统计',
    province VARCHAR(50) COMMENT '省份',
    province_count INT COMMENT '省份统计',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间'
);

-- 索引优化
CREATE INDEX idx_window_time ON user_stats(window_time);
CREATE INDEX idx_action_type ON user_stats(action_type);
CREATE INDEX idx_province ON user_stats(province);
```

### 11.2 历史数据查询API

#### 11.2.1 分页查询接口
```python
@app.route('/api/user_stats')
def api_user_stats():
    """分页查询用户统计数据"""
    try:
        limit = int(request.args.get('limit', 100))  # 默认每页100条
        offset = int(request.args.get('offset', 0))  # 默认从第0条开始
        data = query_user_stats(limit=limit, offset=offset)
        return jsonify({
            'code': 0, 
            'data': data,
            'pagination': {
                'limit': limit,
                'offset': offset,
                'total': len(data)
            }
        })
    except Exception as e:
        logger.error(f"查询用户统计数据失败: {str(e)}")
        return jsonify({'code': 1, 'msg': str(e)})
```

#### 11.2.2 数据库查询函数
```python
def query_user_stats(limit=100, offset=0):
    """查询用户统计数据"""
    sql = """
        SELECT 
            window_time,
            action_type,
            action_count,
            age_range,
            age_count,
            gender,
            gender_count,
            province,
            province_count,
            created_at
        FROM user_stats 
        ORDER BY window_time DESC 
        LIMIT %s OFFSET %s
    """
    try:
        conn = get_connection()
        if conn:
            with conn.cursor() as cursor:
                cursor.execute(sql, (limit, offset))
                result = cursor.fetchall()
            conn.close()
            logger.info(f"成功查询 {len(result)} 条历史数据")
            return result
    except Exception as e:
        logger.error(f"查询用户统计数据失败: {str(e)}")
        return []
```

### 11.3 高级查询功能

#### 11.3.1 时间范围查询
```python
def query_user_stats_by_time_range(start_time, end_time, limit=100):
    """按时间范围查询用户统计数据"""
    sql = """
        SELECT * FROM user_stats 
        WHERE window_time BETWEEN %s AND %s
        ORDER BY window_time DESC 
        LIMIT %s
    """
    try:
        conn = get_connection()
        if conn:
            with conn.cursor() as cursor:
                cursor.execute(sql, (start_time, end_time, limit))
                result = cursor.fetchall()
            conn.close()
            return result
    except Exception as e:
        logger.error(f"时间范围查询失败: {str(e)}")
        return []
```

#### 11.3.2 多维度统计查询
```python
def query_multi_dimension_stats(start_time=None, end_time=None):
    """多维度统计查询"""
    conditions = []
    params = []
    
    if start_time and end_time:
        conditions.append("window_time BETWEEN %s AND %s")
        params.extend([start_time, end_time])
    
    where_clause = " WHERE " + " AND ".join(conditions) if conditions else ""
    
    sql = f"""
        SELECT 
            DATE(window_time) as date,
            action_type,
            SUM(action_count) as total_actions,
            COUNT(DISTINCT province) as province_count,
            AVG(age_count) as avg_age_count
        FROM user_stats 
        {where_clause}
        GROUP BY DATE(window_time), action_type
        ORDER BY date DESC, total_actions DESC
    """
    
    try:
        conn = get_connection()
        if conn:
            with conn.cursor() as cursor:
                cursor.execute(sql, params)
                result = cursor.fetchall()
            conn.close()
            return result
    except Exception as e:
        logger.error(f"多维度统计查询失败: {str(e)}")
        return []
```

### 11.4 数据导出功能

#### 11.4.1 CSV导出
```python
def export_user_stats_to_csv(csv_path='user_stats_export.csv'):
    """导出用户统计数据到CSV文件"""
    sql = """
        SELECT 
            window_time as '统计时间',
            action_type as '行为类型',
            action_count as '行为次数',
            age_range as '年龄段',
            age_count as '年龄统计',
            gender as '性别',
            gender_count as '性别统计',
            province as '省份',
            province_count as '省份统计',
            created_at as '创建时间'
        FROM user_stats 
        ORDER BY window_time DESC
    """
    try:
        conn = get_connection()
        if conn:
            df = pd.read_sql(sql, conn)
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            conn.close()
            logger.info(f"成功导出 {len(df)} 条数据到 {csv_path}")
            return csv_path
    except Exception as e:
        logger.error(f"导出CSV失败: {str(e)}")
        return None
```

#### 11.4.2 Excel导出
```python
def export_user_stats_to_excel(excel_path='user_stats_export.xlsx'):
    """导出用户统计数据到Excel文件"""
    try:
        conn = get_connection()
        if conn:
            # 查询不同维度的数据
            action_sql = "SELECT action_type, SUM(action_count) as total FROM user_stats GROUP BY action_type"
            age_sql = "SELECT age_range, SUM(age_count) as total FROM user_stats GROUP BY age_range"
            gender_sql = "SELECT gender, SUM(gender_count) as total FROM user_stats GROUP BY gender"
            province_sql = "SELECT province, SUM(province_count) as total FROM user_stats GROUP BY province ORDER BY total DESC LIMIT 20"
            
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                # 导出各维度数据到不同sheet
                pd.read_sql(action_sql, conn).to_excel(writer, sheet_name='行为统计', index=False)
                pd.read_sql(age_sql, conn).to_excel(writer, sheet_name='年龄统计', index=False)
                pd.read_sql(gender_sql, conn).to_excel(writer, sheet_name='性别统计', index=False)
                pd.read_sql(province_sql, conn).to_excel(writer, sheet_name='省份统计', index=False)
            
            conn.close()
            logger.info(f"成功导出Excel文件: {excel_path}")
            return excel_path
    except Exception as e:
        logger.error(f"导出Excel失败: {str(e)}")
        return None
```

### 11.5 数据可视化查询

#### 11.5.1 历史趋势图表
```python
def create_historical_trend_chart(start_time=None, end_time=None):
    """创建历史趋势图表"""
    try:
        # 查询历史数据
        data = query_multi_dimension_stats(start_time, end_time)
        
        if not data:
            return None
        
        # 处理数据
        dates = [item['date'] for item in data]
        actions = [item['total_actions'] for item in data]
        
        # 创建趋势图
        line = (
            Line(init_opts=opts.InitOpts(theme="dark"))
            .add_xaxis(dates)
            .add_yaxis("用户行为总数", actions, is_smooth=True)
            .set_global_opts(
                title_opts=opts.TitleOpts(title="用户行为历史趋势"),
                xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=45)),
                yaxis_opts=opts.AxisOpts(name="行为次数"),
                datazoom_opts=[opts.DataZoomOpts()],
                tooltip_opts=opts.TooltipOpts(trigger="axis")
            )
        )
        return json.loads(line.dump_options())
    except Exception as e:
        logger.error(f"创建历史趋势图表失败: {str(e)}")
        return None
```

### 11.6 查询性能优化

#### 11.6.1 数据库索引优化
```sql
-- 为常用查询字段创建索引
CREATE INDEX idx_window_time_action ON user_stats(window_time, action_type);
CREATE INDEX idx_province_time ON user_stats(province, window_time);
CREATE INDEX idx_created_at ON user_stats(created_at);

-- 复合索引优化
CREATE INDEX idx_multi_query ON user_stats(window_time, action_type, province);
```

#### 11.6.2 查询缓存机制
```python
from functools import lru_cache
import time

@lru_cache(maxsize=128)
def cached_query_user_stats(limit, offset, cache_key=None):
    """带缓存的用户统计查询"""
    if cache_key is None:
        cache_key = f"{limit}_{offset}_{int(time.time() / 300)}"  # 5分钟缓存
    
    return query_user_stats(limit, offset)
```

### 11.7 历史数据管理

#### 11.7.1 数据清理策略
```python
def cleanup_old_data(days_to_keep=90):
    """清理过期历史数据"""
    sql = "DELETE FROM user_stats WHERE created_at < DATE_SUB(NOW(), INTERVAL %s DAY)"
    try:
        conn = get_connection()
        if conn:
            with conn.cursor() as cursor:
                cursor.execute(sql, (days_to_keep,))
                deleted_count = cursor.rowcount
                conn.commit()
            conn.close()
            logger.info(f"成功清理 {deleted_count} 条过期数据")
            return deleted_count
    except Exception as e:
        logger.error(f"清理过期数据失败: {str(e)}")
        return 0
```

#### 11.7.2 数据备份策略
```python
def backup_user_stats(backup_path=None):
    """备份用户统计数据"""
    if backup_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = f"backup/user_stats_{timestamp}.csv"
    
    try:
        success = export_user_stats_to_csv(backup_path)
        if success:
            logger.info(f"数据备份成功: {backup_path}")
            return backup_path
        else:
            logger.error("数据备份失败")
            return None
    except Exception as e:
        logger.error(f"数据备份异常: {str(e)}")
        return None
```

## 12. API接口文档

### 12.1 接口概述

淘宝实时数据分析平台提供了完整的REST API和WebSocket API接口，支持实时数据查询、历史数据检索、数据导出等功能。

#### 12.1.1 基础信息
- **基础URL**: `http://hadoop105:5001`
- **API版本**: v1.0
- **数据格式**: JSON
- **字符编码**: UTF-8

#### 12.1.2 响应格式
```json
{
    "code": 0,           // 状态码：0-成功，1-失败
    "msg": "success",    // 响应消息
    "data": {},          // 响应数据
    "timestamp": 1640995200000  // 时间戳
}
```

### 12.2 REST API接口

#### 12.2.1 首页接口
```http
GET /
```

**描述**: 获取系统首页，展示实时数据大屏

**响应**: HTML页面

**示例**:
```bash
curl -X GET http://hadoop105:5001/
```

#### 12.2.2 用户统计数据查询接口
```http
GET /api/user_stats
```

**描述**: 分页查询用户统计数据

**请求参数**:
| 参数名 | 类型 | 必填 | 默认值 | 说明 |
|--------|------|------|--------|------|
| limit | int | 否 | 100 | 每页记录数，最大1000 |
| offset | int | 否 | 0 | 偏移量，从0开始 |

**响应示例**:
```json
{
    "code": 0,
    "msg": "success",
    "data": [
        {
            "id": 1,
            "window_time": "2024-01-01 10:00:00",
            "action_type": "点击",
            "action_count": 1500,
            "age_range": "18-24岁",
            "age_count": 800,
            "gender": "女性",
            "gender_count": 900,
            "province": "北京",
            "province_count": 200,
            "created_at": "2024-01-01 10:05:00"
        }
    ],
    "pagination": {
        "limit": 100,
        "offset": 0,
        "total": 1500
    }
}
```

#### 12.2.3 数据导出接口
```http
GET /api/export_user_stats
```

**描述**: 导出用户统计数据为CSV文件

**请求参数**: 无

**响应**: CSV文件下载

**响应头**:
```
Content-Type: text/csv
Content-Disposition: attachment; filename=user_stats_export.csv
```

**示例**:
```bash
curl -X GET http://hadoop105:5001/api/export_user_stats -o user_stats.csv
```

### 12.3 WebSocket API接口

#### 12.3.1 连接管理

##### 连接建立
```javascript
// 客户端连接
const socket = io('http://hadoop105:5001');

socket.on('connect', function() {
    console.log('Connected to server');
});
```

**事件**: `connect`

**描述**: 客户端成功连接到服务器

**服务器响应**: 发送初始空数据
```json
{
    "action": {},
    "age": {},
    "gender": {},
    "province": {}
}
```

##### 连接断开
```javascript
socket.on('disconnect', function() {
    console.log('Disconnected from server');
});
```

**事件**: `disconnect`

**描述**: 客户端断开连接

#### 12.3.2 图表数据请求

##### 请求图表数据
```javascript
// 客户端发送请求
socket.emit('request_charts', {});
```

**事件**: `request_charts`

**描述**: 请求当前图表数据

**请求参数**: 空对象 `{}`

**服务器响应**: `update_charts`
```json
{
    "action": {
        "title": {"text": "用户行为分布 (Flink)"},
        "series": [{
            "data": [
                {"name": "点击", "value": 1500},
                {"name": "加入购物车", "value": 800},
                {"name": "购买", "value": 300},
                {"name": "关注商品", "value": 200}
            ]
        }]
    },
    "age": {
        "title": {"text": "年龄分布 (Flink)"},
        "xAxis": {"data": ["18-24岁", "25-29岁", "30-34岁"]},
        "series": [{"data": [800, 600, 400]}]
    },
    "gender": {
        "title": {"text": "性别分布 (Flink)"},
        "series": [{
            "data": [
                {"name": "女性", "value": 900},
                {"name": "男性", "value": 700},
                {"name": "未知", "value": 100}
            ]
        }]
    },
    "province": {
        "title": {"text": "省份分布 (Flink)"},
        "xAxis": {"data": ["北京", "上海", "广东", "浙江"]},
        "series": [{"data": [200, 180, 150, 120]}]
    }
}
```

#### 12.3.3 实时数据更新

##### 数据更新推送
**事件**: `update_charts`

**描述**: 服务器主动推送实时更新的图表数据

**触发条件**:
- Flink处理新数据后
- 统计数据发生变化时

**数据格式**: 与 `request_charts` 响应格式相同

### 12.4 高级查询API

#### 12.4.1 时间范围查询接口
```http
GET /api/user_stats/range
```

**描述**: 按时间范围查询用户统计数据

**请求参数**:
| 参数名 | 类型 | 必填 | 说明 |
|--------|------|------|------|
| start_time | string | 是 | 开始时间，格式：YYYY-MM-DD HH:MM:SS |
| end_time | string | 是 | 结束时间，格式：YYYY-MM-DD HH:MM:SS |
| limit | int | 否 | 返回记录数限制 |

**响应示例**:
```json
{
    "code": 0,
    "msg": "success",
    "data": [
        {
            "date": "2024-01-01",
            "action_type": "点击",
            "total_actions": 5000,
            "province_count": 25,
            "avg_age_count": 1200
        }
    ]
}
```

#### 12.4.2 多维度统计接口
```http
GET /api/user_stats/dimensions
```

**描述**: 获取多维度统计数据

**请求参数**:
| 参数名 | 类型 | 必填 | 说明 |
|--------|------|------|------|
| dimension | string | 是 | 统计维度：action/age/gender/province |
| group_by | string | 否 | 分组字段 |
| order_by | string | 否 | 排序字段 |
| limit | int | 否 | 返回记录数限制 |

**响应示例**:
```json
{
    "code": 0,
    "msg": "success",
    "data": {
        "dimension": "action",
        "statistics": [
            {"action_type": "点击", "count": 5000, "percentage": 65.2},
            {"action_type": "加入购物车", "count": 1800, "percentage": 23.4},
            {"action_type": "购买", "count": 600, "percentage": 7.8},
            {"action_type": "关注商品", "count": 300, "percentage": 3.6}
        ]
    }
}
```

### 12.5 数据导出API

#### 12.5.1 Excel导出接口
```http
GET /api/export_user_stats/excel
```

**描述**: 导出用户统计数据为Excel文件

**请求参数**:
| 参数名 | 类型 | 必填 | 说明 |
|--------|------|------|------|
| start_time | string | 否 | 开始时间 |
| end_time | string | 否 | 结束时间 |
| sheets | string | 否 | 指定导出的sheet，逗号分隔 |

**响应**: Excel文件下载

**响应头**:
```
Content-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
Content-Disposition: attachment; filename=user_stats_export.xlsx
```

#### 12.5.2 JSON导出接口
```http
GET /api/export_user_stats/json
```

**描述**: 导出用户统计数据为JSON文件

**请求参数**:
| 参数名 | 类型 | 必填 | 说明 |
|--------|------|------|------|
| pretty | boolean | 否 | 是否格式化输出，默认true |

**响应示例**:
```json
{
    "code": 0,
    "msg": "success",
    "data": {
        "filename": "user_stats_20240101.json",
        "download_url": "/downloads/user_stats_20240101.json",
        "record_count": 1500,
        "export_time": "2024-01-01 10:30:00"
    }
}
```

### 12.6 系统监控API

#### 12.6.1 系统状态接口
```http
GET /api/system/status
```

**描述**: 获取系统运行状态

**响应示例**:
```json
{
    "code": 0,
    "msg": "success",
    "data": {
        "system": {
            "status": "running",
            "uptime": 86400,
            "version": "1.0.0",
            "start_time": "2024-01-01 00:00:00"
        },
        "components": {
            "kafka": {"status": "connected", "topic": "taobao_data2"},
            "flink": {"status": "running", "job_id": "job_123456"},
            "database": {"status": "connected", "pool_size": 5},
            "websocket": {"status": "active", "connections": 10}
        },
        "performance": {
            "cpu_usage": 45.2,
            "memory_usage": 68.5,
            "disk_usage": 23.1,
            "network_io": 1024.5
        }
    }
}
```

#### 12.6.2 性能指标接口
```http
GET /api/system/metrics
```

**描述**: 获取系统性能指标

**请求参数**:
| 参数名 | 类型 | 必填 | 说明 |
|--------|------|------|------|
| metric | string | 否 | 指标类型：cpu/memory/disk/network/all |
| period | string | 否 | 时间周期：1m/5m/1h/1d |

**响应示例**:
```json
{
    "code": 0,
    "msg": "success",
    "data": {
        "timestamp": 1640995200000,
        "metrics": {
            "cpu": {
                "usage": 45.2,
                "load_avg": [1.2, 1.1, 0.9]
            },
            "memory": {
                "total": 8589934592,
                "used": 5899345920,
                "free": 2690588672,
                "usage_percent": 68.5
            },
            "disk": {
                "total": 107374182400,
                "used": 24748364800,
                "free": 82625817600,
                "usage_percent": 23.1
            },
            "network": {
                "bytes_sent": 1024000,
                "bytes_recv": 2048000,
                "packets_sent": 1000,
                "packets_recv": 2000
            }
        }
    }
}
```

### 12.7 错误码说明

| 错误码 | 说明 | 解决方案 |
|--------|------|----------|
| 0 | 成功 | - |
| 1 | 一般错误 | 查看错误消息 |
| 1001 | 参数错误 | 检查请求参数 |
| 1002 | 数据库连接失败 | 检查数据库配置 |
| 1003 | 查询超时 | 优化查询或增加超时时间 |
| 1004 | 文件导出失败 | 检查文件权限和磁盘空间 |
| 1005 | WebSocket连接失败 | 检查网络连接 |
| 1006 | 数据格式错误 | 检查数据格式 |
| 1007 | 权限不足 | 检查用户权限 |
| 1008 | 服务不可用 | 检查服务状态 |

### 12.8 API使用示例

#### 12.8.1 Python客户端示例
```python
import requests
import json
import socketio

# REST API调用
def get_user_stats(limit=100, offset=0):
    url = "http://hadoop105:5001/api/user_stats"
    params = {"limit": limit, "offset": offset}
    
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"API调用失败: {e}")
        return None

# WebSocket客户端
def websocket_client():
    sio = socketio.Client()
    
    @sio.event
    def connect():
        print("Connected to server")
        sio.emit('request_charts', {})
    
    @sio.on('update_charts')
    def on_update_charts(data):
        print("收到图表更新:", json.dumps(data, indent=2))
    
    @sio.event
    def disconnect():
        print("Disconnected from server")
    
    try:
        sio.connect('http://hadoop105:5001')
        sio.wait()
    except Exception as e:
        print(f"WebSocket连接失败: {e}")

# 使用示例
if __name__ == "__main__":
    # 查询用户统计数据
    stats = get_user_stats(limit=50)
    if stats and stats['code'] == 0:
        print(f"查询到 {len(stats['data'])} 条记录")
    
    # 连接WebSocket
    websocket_client()
```

#### 12.8.2 JavaScript客户端示例
```javascript
// REST API调用
async function getUserStats(limit = 100, offset = 0) {
    try {
        const response = await fetch(`http://hadoop105:5001/api/user_stats?limit=${limit}&offset=${offset}`);
        const data = await response.json();
        
        if (data.code === 0) {
            console.log('查询成功:', data.data);
            return data.data;
        } else {
            console.error('查询失败:', data.msg);
            return null;
        }
    } catch (error) {
        console.error('API调用异常:', error);
        return null;
    }
}

// WebSocket客户端
const socket = io('http://hadoop105:5001');

socket.on('connect', function() {
    console.log('已连接到服务器');
    // 请求图表数据
    socket.emit('request_charts', {});
});

socket.on('update_charts', function(data) {
    console.log('收到图表更新:', data);
    // 更新页面图表
    updateCharts(data);
});

socket.on('disconnect', function() {
    console.log('与服务器断开连接');
});

// 更新图表函数
function updateCharts(data) {
    // 更新用户行为分布图
    if (data.action) {
        updateActionChart(data.action);
    }
    
    // 更新年龄分布图
    if (data.age) {
        updateAgeChart(data.age);
    }
    
    // 更新性别分布图
    if (data.gender) {
        updateGenderChart(data.gender);
    }
    
    // 更新省份分布图
    if (data.province) {
        updateProvinceChart(data.province);
    }
}

// 使用示例
document.addEventListener('DOMContentLoaded', function() {
    // 页面加载时查询历史数据
    getUserStats(50, 0);
});
```

### 12.9 API限流和认证

#### 12.9.1 限流策略
- **REST API**: 每分钟最多1000次请求
- **WebSocket**: 每个IP最多10个并发连接
- **数据导出**: 每小时最多10次导出请求

#### 12.9.2 认证机制
```python
# 认证中间件示例
def require_auth(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        token = request.headers.get('Authorization')
        if not token or not validate_token(token):
            return jsonify({'code': 1007, 'msg': '权限不足'}), 401
        return f(*args, **kwargs)
    return decorated_function

# 使用认证装饰器
@app.route('/api/admin/stats')
@require_auth
def admin_stats():
    # 管理员专用接口
    pass
```

## 13. 监控和日志

### 13.1 日志配置
```python
# 日志配置
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
```

### 13.2 性能监控
- **处理速率监控**：实时监控数据处理速率
- **延迟监控**：监控数据处理的延迟情况
- **错误率监控**：监控系统错误率
- **资源使用监控**：监控CPU、内存使用情况

### 13.3 告警机制
- **队列积压告警**：当队列积压超过阈值时告警
- **处理延迟告警**：当处理延迟超过阈值时告警
- **错误率告警**：当错误率超过阈值时告警

## 14. 部署和运维

### 14.1 环境要求
- **Python版本**：3.7+
- **Java版本**：8+
- **Flink版本**：1.17.1
- **Kafka版本**：2.8+
- **MySQL版本**：5.7+

### 14.2 部署步骤
1. **环境准备**：安装Python、Java、Flink、Kafka、MySQL
2. **依赖安装**：`pip install -r requirements.txt`
3. **配置修改**：修改配置文件中的连接信息
4. **服务启动**：按顺序启动Zookeeper、Kafka、Flink、应用服务

### 14.3 运维管理
- **服务监控**：监控各组件服务状态
- **日志管理**：定期清理和归档日志文件
- **备份策略**：定期备份配置和数据
- **扩容方案**：支持水平扩展和负载均衡

## 15. 性能优化

### 15.1 系统性能指标
- **吞吐量**：支持每秒处理1000+条消息
- **延迟**：端到端延迟控制在100ms以内
- **可用性**：系统可用性达到99.9%
- **扩展性**：支持水平扩展，线性提升性能

### 15.2 优化策略
- **并行度调优**：根据数据量调整Flink并行度
- **内存优化**：合理配置JVM内存参数
- **网络优化**：优化Kafka网络配置
- **存储优化**：使用SSD提升I/O性能

## 16. 故障排查

### 16.1 常见问题
- **Kafka连接失败**：检查网络连接和配置
- **Flink任务失败**：查看日志和检查点状态
- **WebSocket连接断开**：检查网络和防火墙设置
- **数据库连接超时**：检查连接池配置

### 16.2 排查工具
- **日志分析**：使用ELK栈进行日志分析
- **性能监控**：使用Prometheus + Grafana监控
- **链路追踪**：使用Jaeger进行分布式追踪
- **健康检查**：定期进行系统健康检查

## 17. 安全考虑

### 17.1 数据安全
- **数据加密**：传输和存储数据加密
- **访问控制**：基于角色的访问控制
- **审计日志**：记录所有操作日志
- **数据脱敏**：敏感数据脱敏处理
